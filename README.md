# Project-7---News-Classifier
This project implements a news‑topic classification pipeline using the AG News dataset. Text is cleaned, tokenized, and converted into sequences, then split 80:20 into training and test sets. Two deep‑learning architectures are evaluated: a bidirectional LSTM trained end‑to‑end, and a pretrained BERT encoder fine‑tuned with AdamW and a cosine learning‑rate scheduler. Model performance is quantified using accuracy, precision, recall, F1 score, and confusion matrix. The BERT model substantially outperforms the LSTM by leveraging contextual embeddings, highlighting the advantages of transfer learning for robust topical categorization in natural language processing.
